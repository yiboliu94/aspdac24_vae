\section{Proposed VAE-accelerated EM-aware IR-drop fixing method}
\label{sec:strategy}


%First we introduce our VAE-based full-chip EM-aware IR-drop prediction model, the data preparation, and training.
%Then we introduce how to utilize the auto-gradient from VAE model to accelerate the convention SLP power grid fixing method ~\cite{Sukharev:2019pg}.



\subsection{VAE-based EM-aware IR-drop prediction model}
\label{subsec:vae}


%The Generative Adversarial Network (GAN) is a form of neural network model utilized in unsupervised machine learning, originally developed by Ian Goodfellow. A standard GAN consists of two distinct deep neural networks - the generator G and the %discriminator D. G is tasked with producing outputs closely resembling the dataset used for training, whereas D is responsible for discerning between actual and artificially generated data. The generator within a typical GAN employs a random noise vector z as %its input, converting it to output G(z). The discriminator, a deep binary classifier, accepts both genuine and manufactured data alternately, offering a "score". This score is the basis for classifying data as "authentic" or "synthetic", and is also a component of the %loss function, used to train both G and D through back propagation. Training both networks is conducted concurrently and alternately to avoid one trailing significantly behind the other until equilibrium is achieved. In comparison, a regular Convolutional Neural %Network (CNN) utilizes a simple least square loss function, whereas the loss function of a GAN is a combination of the least square loss between prediction and actual data, and a discriminator's score. Thus, a GAN can be perceived as an advanced CNN with %improved accuracy.

%The Conditional Generative Adversarial Network (CGAN) operates within conditional settings, learning a conditional generative model. In contrast to a standard GAN, the generator's input in a CGAN is a fusion of both a condition vector x and a random noise %vector z, with the output labelled as G(x, z). The fundamental disparity with GAN lies in the fact that both G and D in a CGAN are conditional on vector x. CGANs have demonstrated outstanding performance in image-to-image translation tasks, where the input %image acts as the condition, directing the image generation process. In our study, the EM stress distribution generated is contingent on the input current density and given aging time, following the physics-law of stress evolution, making the CGAN model %exceptionally suitable.



We use VAE as the backbone for our EM-aware IR-drop prediction model. The structure is shown in Fig.\ref{fig:VAE_architecture}, and we use a power grid with 64 rows and 64 columns as an example.

\begin{figure*}[h!]
	\centering
	\includegraphics[width=1.8\columnwidth]{./figs/vae_detail.eps}
	\caption{VAE-based model for EM-aware IR drop prediction} 
	\label{fig:VAE_architecture}
\end{figure*}


The model input consists of four channels. They are the power grid branch conductance splitted into vertical conductance $\mathbb{R} ^{64\times 64\times 1}$ and horizontal conductance $\mathbb{R} ^{64\times 64\times 1}$, the current source $\mathbb{R} ^{64\times 64\times 1}$ drawn to other circuit layers, and a target life time $\textit{\textbf{t}} $expanded into $\mathbb{R} ^{64\times 64\times 1}$ by channel-wise duplication as the conditional information. Hence the total input will be 64x64x4. 
The conductance and current are normalized, then the input $\textit{\textbf{x}}$ is sent into the encoder and encoded as $ \mu_{x} $  and $ \sigma_{x}$. 
In this example, the encoder contains of four convolutional layers followed by two fully-connected layers. The detailed dimensions of the layer have been indicated in Fig.\ref{fig:VAE_architecture}. 
Next we use the reparameterization trick as described in \ref{subsec:vae_intro}  to sample and obtain the latent variable $\textbf{z}$ and send it to the decoder.
The decoder is designed to mirror the architecture of the encoder in a symmetric manner, except the output layer has only one channel. The output is $\mathbb{R} ^{64\times 64\times 1}$ EM aware IR drop at target aging year.


If The input is first padded to the standard size of 64 x 64 for the power grid smaller than this size. If the power grid exceeds the size of 64 x 64 but smaller than 128 x 128, then we add one more convolutional layer in both the generator and the discriminator network. 
For the further larger or smaller sized power grid, we follow the same routine to add or remove the convolutional layers to fit the input size.


During training, the VAE learns to map the input data to a lower-dimensional latent space in a way that it can generate similar data from points in this space. Because of the probabilistic nature of the encoding, and the use of the KL divergence in the loss function, the VAE is encouraged to create a smooth and continuous latent space, where similar points decode to similar data.






As for our VAE-based model training data, first we extract the electrical information from the result of the  $\it{EMspice}$. The input features of VAE model should include the node voltage $u(t)$ and $M(t)$,  the conductance matrix of the power grid network.
%in the format of wire segment resistance vectors, which are mentioned in Eq.~\eqref{eq:mna}. 


The data preprocessing before the model training is as follows. Synopsys IC compiler can automatically create the circuit layout from a synthesized gate-level netlist and a standard cell library. 
Then we parse the electrical features and the topological information from the circuit layout.

\begin{table}[!htbp]
	\begin{center}
		\caption{Power Grid Design Detail}
		\label{table:pre_results}
		%\vspace{-0.1in}
		\center
		\resizebox{0.48\textwidth}{!}{
		\begin{tabular}{ c | c | c | c | c}
			\hline 
			circuit &{\# nodes}&{\# Trees}&{\# voltage sources} &{$V_{DD}$ (V)}  \\ \hline 
			\hline 
			Design1 &1024 &64   &2 &1.05  \\ \hline
			Design2 &4096 &128  &4 &1.05 \\ \hline
			Design3 &16384 &256 &4 &1.05  \\ \hline
		\end{tabular}
		}
	\end{center}
	\vspace{-0.1in}
\end{table}




\subsection{power grid EM-aware IR drop fixing framework }
\label{subsec:formulation}

The structure of the combined VAE-based model and the proposed workflow of the fixing strategy is shown in Fig.~\ref{fig:flow}. 

%\begin{figure*}[h!]
%	\centering
%	\captionsetup{justification=centering, margin=3cm}
%	\includegraphics[width=1.2\columnwidth]{./figs/flow.eps}
%	\caption{The proposed framework of VAE-accelerated power gird fixing method.}
%	\label{fig:flow}
%\end{figure*}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.98\columnwidth]{./figs/flow.eps}
	\caption{Framework of power gird IR drop fixing method accelerated by the VAE model.}
	\label{fig:flow}
\end{figure}

\subsubsection{problem formulation}
\label{subsubsec:formulation}
The proposed method intends to solve the following problem: 
Given the power grid information at T=0, our VAE-based model has predict the EM-aware IR drop the target aging life time \textit{\textbf{t}} in ~\ref{subsec:vae}.The power grid has EM-induced IR drop violations and demands fixing if it has node voltage drop \textit{\textbf{v}}  above the threshold $V_{th}$.
We wish to alleviate the EM-aware IR drop failure by resizing the power grid interconnect trees width with minimum metal area increase.

The problem can be formulated as:
\begin{align}
	\label{eq:prob_formulation}
	&\mbox{Minimize}  & a^{t}s \qquad   \notag  \\
	&s.t.     & v(t,s)\leq V_{th} \\
	& \quad   &s \in S   \triangleq \{ s \in R^{nt}: 1 \leq s \leq s_{up} \}        \notag
\end{align}

In \eqref{eq:prob_formulation}, $a=[a_{1},a_{2},\ldots,a_{nt}] = [w_{1}l_{1},w_{2}l_{2},\ldots,w_{nt}l_{nt}]$ refers to the metal areas of power grid with $n_{t}$ trees, $w_{i}$ and $l_{i}$ are the $i_{th}$ tree's width and length separately.
For the constraints, $s=[s_{1},s_{2},\ldots,s_{nt}]$  is the resizing factor for the power grid trees. $S$ is the feasible region ,
where $s_{up}$ is the upper bound for $s$. Similar to~\cite{Sukharev:2019pg}, we assume the original power grid trees are already set to its minimum width. Hence we only increase the tree width and $s \geq 1 $.
The feasible region reflects both the basic design rules and case-by-case user requirements, such as the criteria of minimum interconnect tree width, the minimum spacing to prevent the  interconnect trees overlap, and the maximum metal area usage, etc. 
The node IR drop $v(t,s)$  is the voltage difference between the power grid interconnect node and the power supply, its is a nonlinear function to $s$ at aging time $t$. The maximum allowable voltage drop threshold $V_{th}$ is given by user, we assumed it to be ten percents of the power supply voltage.


\subsubsection{Programming-based optimization}
\label{subsubsec:slp_framework}

As we mentioned in ~\ref{subsubsec:formulation}, the EM-aware PG node voltage drop $v(t,s)$ is a nonlinear function to $s$ at aging time $t$, hence \eqref{eq:prob_formulation} is a nonlinear optimization problem. 
We solve it by a stepping strategy, which we linearize the voltage drop at current latest solution point by Taylor's expansion \eqref{eq:v_taylor_expand} , and solve it with linear programming (LP) solver. We repeat this operation until the power grid has no EM-aware IR drop violation.

\begin{equation}
	\label{eq:v_taylor_expand}
	v(t, s^{(i+1)}) \triangleq v(t,s^{(i)}) + \dfrac{\partial v(t, s^{(i)})}{\partial s} \cdot \delta s
\end{equation}
where $s^{(i)}$ denotes the current power grid resizing vector and $s^{(i+1)}$ is defined as 
\begin{equation}
	\label{eq:s}
	s^{(i+1)} = s^{(i)} + \delta s 
\end{equation}

$ \dfrac{\partial v(t, s)}{\partial s}$ is the $n\times n_{t}$ \textit{Jacobian} matrix of $v(t,s)$ with respect to $s$, describes how the node voltage drops at target time $T=t$ respond to the corresponding tree width rescaling.

\begin{equation}
	\label{eq:J_matrix}
	\dfrac{\partial v(t, s)}{\partial s}=
	\mathbf {J}_{n\times n_{t}}(s,t) =
	\begin{bmatrix}
		\frac{\partial v(1,t)}{\partial s_{1}}&\frac{\partial v(1,t)}{\partial s_{2}}&\ldots&\frac{\partial v(1,t)}{\partial s_{n_{t}}}\\
		\frac{\partial v(2,t)}{\partial s_{1}}&\frac{\partial v(2,t)}{\partial s_{2}}&\ldots&\frac{\partial v(2,t)}{\partial s_{n_{t}}}\\
		\vdots&\vdots&\ddots&\vdots\\
		\frac{\partial v(n,t)}{\partial s_{1}}&\frac{\partial v(n,t)}{\partial s_{2}}&\ldots&\frac{\partial v(n,t)}{\partial s_{n_{t}}}
	\end{bmatrix}
\end{equation}



\subsubsection{Fast Gradient acquisition}
PyTorch auto differentiation can provide the gradient of output node voltages to the input wire segment conductances, which is $ \dfrac{\partial v(t, s)}{\partial g}$. Then we can easily get \eqref{eq:J_matrix} by chain rule, as each power grid tree $g_{i}$ has its own resizing factor $s_{i}$ and will not be infected by other resizing factors:
\begin{equation}
	\label{eq:chain_rule}
	\begin{aligned}
	\begin{split}
	\frac{\partial v}{\partial s} & =\frac{\partial v}{\partial g} \frac{\partial g}{\partial s}, \\ 
	\frac{\partial g_{i}}{\partial s_{k}} & = 
    	\begin{cases}
        		0,      &\mbox{if i $\neq$ k} \\ 
        		g_{i},  &\mbox{if i = k} 
    	\end{cases}
	\end{split}
	\end{aligned}
\end{equation}



As Comparison, we compare to the sensitivity ~\cite{Sukharev:2019pg}
For a the power grid network represented by the node conductance matrix $ \textit{G(t,s)}$, we can write its model in the format of the following equation:
\begin{equation}
	\label{eq:gv=i}
	G(t,s)\cdot v(t,s)= j(t)
\end{equation}
where $j(t)$  is $n\times 1$ vectors representing and node current vector for the power grid.

And the sensitivity 
\begin{equation}
	\label{eq:dVs}
	\dfrac{\partial v(t,s)}{\partial s_{k}} = -G^{-1}\cdot \dfrac{\partial G(t,s)}{\partial s_{k}}  \cdot G^{-1}\cdot j(t)
\end{equation}


To solve the above equation and obtain $ \frac{\partial v(t,s)}{\partial s_{k}}$, one has to first construct the sparse matrices G and $\frac{\partial G(t,s)}{\partial s_{k}}$ for all $k$, and then perform matrix solving. 
Each column of the  \textit{Jacobian} matrix \eqref{eq:J_matrix} has to be calculated through the process of \eqref{eq:dVs} for the corresponding power grid tree. Hence building such a \textit{Jacobian} matrix in traditional method is computational expensive.






%We utilize the auto differentiation to calculate the sensitivity of output node voltage to the input power grid wire segment resistance, as VAE models are differentiable with respect to the model parameters. 
%First we train the VAE model extracted from the analytical results from $\it{EMspice}$, which provides the node voltage and branch resistance information of the power grid from $T = 0$ to $T = T_{target}$. 


%The output of the VAE mode contains two parts, the first part is the power grid nodes voltage at the target aging time considering the EM-induced aging effect, the second part is the sensitivity(gradient) of the output (voltage) with respect to the inputs (such as the resizing factor $s_{k}$ for the $k_{th}$ tree).  Such gradient acquisition can be very efficient as we can compute all the sensitivities just by one back propagation, and all the local gradients have been computed already. The acquirement of such sensitivity in our method utilizes the inherent characteristics of the differentiable VAE model. Once the models have been trained, they have the promise to be applied to the different designs, specially by using localized training as shown in recent work~\cite{WenPan:SemiTherm'2020}. 








